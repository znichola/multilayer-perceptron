{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065247b3-67f8-461d-9ef6-47cc9a4688ca",
   "metadata": {},
   "source": [
    "how to solve this thing?\n",
    "\n",
    "I need to do the process of a nural net. It's not that complicated but requiers a bit of finageling.\n",
    "\n",
    "We start with splitting the data into train and validate sets.\n",
    "\n",
    "Then we need to create the nural net that will run the calculations.\n",
    "\n",
    "We will copy the pytorch interface\n",
    "\n",
    "you have a Modle class, this encapuslates everything\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038753f-f084-469d-a01d-fa689167c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import common as cm\n",
    "\n",
    "Array = NDArray[np.float32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract class to reprisent a layer in the nural net\n",
    "class Layer(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Array) -> Array:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad_output: Array) -> Array:\n",
    "        pass\n",
    "\n",
    "# Linear transform for incomming data\n",
    "#   y = x @ W + b\n",
    "#\n",
    "# \n",
    "# x = input   shape(num_rows, in_features)\n",
    "# W = weights shape(in_features, out_features)\n",
    "# b = bias    shape(out_features,)\n",
    "# y = output  shape(num_rows, out_features)\n",
    "#\n",
    "# ^1 the @ is matrix multiply same simbole is used in numpy\n",
    "#\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "\n",
    "        # Init weights using Xavier Initialization\n",
    "        limit = 1 / np.sqrt(in_features)\n",
    "        self.W: Array = np.random.uniform(-limit, limit, (in_features, out_features)).astype(np.float32)\n",
    "        \n",
    "        # Init bias to zero\n",
    "        self.b: Array = np.zeros(out_features, dtype=np.float32)\n",
    "\n",
    "        # gradients\n",
    "        self.dW: Array = np.zeros_like(self.W)\n",
    "        self.db: Array = np.zeros_like(self.b)\n",
    "\n",
    "        self.x: Array | None = None  # cache input for backward pass\n",
    "\n",
    "    def forward(self, x: Array) -> Array:\n",
    "        self.x = x\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def backward(self, grad_output: Array) -> Array:\n",
    "        assert self.x is not None\n",
    "        # gradients are stored so the optimiser can read them\n",
    "        # later to adjust them using the learing reate\n",
    "        self.dW = self.x.T @ grad_output  # gradient of loss with respect to (wrt) weights \n",
    "        self.db = grad_output.sum(axis=0) # gradient of loss wrt bias\n",
    "        # gradient wrt input\n",
    "        return grad_output @ self.W.T\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        self.mask: NDArray[np.bool] | None = None\n",
    "    \n",
    "    def forward(self, x: Array) -> Array:\n",
    "        self.mask = x > 0 # mask for gradients above zero, \n",
    "        return np.maximum(x, 0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
